{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhPm_3tHt4mQ",
        "outputId": "8120ebd2-e212-451e-ddf2-cec4c48cf8f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "## for google colab\n",
        "\n",
        "# If encounter google drive issue, remount\n",
        "# !rm -rf /content/drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11eC2IiTuLL6"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install mne ipywidgets ipyevents nibabel trame trame-vtk trame-vuetify pyvista nilearn PyWavelets seaborn statsmodels mne_nirs snirf optuna sktime scikit-learn numpy pandas seaborn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8463p6_t4mT"
      },
      "source": [
        "## 1. Load Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bjepRqht4mU"
      },
      "outputs": [],
      "source": [
        "# Import common libraries\n",
        "from collections import defaultdict\n",
        "from copy import deepcopy\n",
        "from itertools import compress\n",
        "from pprint import pprint\n",
        "\n",
        "# Import Plotting Library\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Import StatsModels\n",
        "import statsmodels.formula.api as smf\n",
        "from mne import Epochs, events_from_annotations, set_log_level\n",
        "from mne.preprocessing.nirs import (\n",
        "    beer_lambert_law,\n",
        "    optical_density,\n",
        "    scalp_coupling_index,\n",
        "    temporal_derivative_distribution_repair,\n",
        ")\n",
        "\n",
        "# Import MNE processing\n",
        "from mne.viz import plot_compare_evokeds\n",
        "\n",
        "\n",
        "# Import MNE-NIRS processing\n",
        "from mne_nirs.channels import get_long_channels, picks_pair_to_idx\n",
        "from mne_nirs.datasets import fnirs_motor_group\n",
        "from mne_nirs.signal_enhancement import enhance_negative_correlation\n",
        "\n",
        "# Set general parameters\n",
        "set_log_level(\"WARNING\")  # Don't show info, as it is repetitive for many subjects\n",
        "from itertools import compress\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import mne\n",
        "import snirf\n",
        "\n",
        "from scipy import signal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdmfGB71t4mU"
      },
      "source": [
        "## 2. Define function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThZGa0b4t4mV"
      },
      "source": [
        "#### 2.0 Remove close channels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zkbin2zlt4mV"
      },
      "outputs": [],
      "source": [
        "def remove_close_channels(raw_intensity):\n",
        "# Remove channels that are too close to each other (less than 1cm)\n",
        "    picks = mne.pick_types(raw_intensity.info, meg=False, fnirs=True) # [0 ... 111]\n",
        "    dists = mne.preprocessing.nirs.source_detector_distances(\n",
        "        raw_intensity.info, picks=picks\n",
        "    )\n",
        "    raw_intensity.pick(picks[dists > 0.01]) # Number of channels with distance > 1 cm: 96\n",
        "\n",
        "    return raw_intensity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzFo5Ctxt4mW"
      },
      "source": [
        "#### 2.1 Annotation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHeqs7m6t4mW"
      },
      "outputs": [],
      "source": [
        "def process_annotations(raw_intensity):\n",
        "\n",
        "    # Define wanted stimuli\n",
        "    wanted_stimulus = [\"2\", \"3\"]\n",
        "    unwanted = np.nonzero(~np.isin(raw_intensity.annotations.description, wanted_stimulus))\n",
        "    # The ~ operator is used to negate the result of np.isin\n",
        "    raw_intensity.annotations.delete(unwanted)\n",
        "    raw_intensity.annotations.rename(\n",
        "        {\"2\": \"Reappraise\", \"3\": \"view negative\"}\n",
        "    )\n",
        "\n",
        "    return raw_intensity\n",
        "\n",
        "# Usage:\n",
        "# raw_intensity = process_annotations(raw_intensity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Eva31tat4mW"
      },
      "source": [
        "#### 2.2 Wavelet motion Correction function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGTuhG-Ct4mW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pywt\n",
        "import mne\n",
        "\n",
        "def hmr_motion_correct_wavelet_standard(raw_od, iqr=1.5, turnon=1, wavename='db2'):\n",
        "    if iqr < 0 or turnon == 0:\n",
        "        return raw_od\n",
        "\n",
        "    data = raw_od.get_data()\n",
        "    dod_wavelet = np.zeros_like(data)\n",
        "    signal_length = data.shape[1]\n",
        "    n_levels = max(int(np.ceil(np.log2(signal_length))) - 4, 4) # decomposition level, L = 4 is the lowest wavelet scale used in the analysis\n",
        "\n",
        "    # Identify bad channels\n",
        "    bads = raw_od.info['bads']\n",
        "    bad_indices = [raw_od.ch_names.index(bad) for bad in bads if bad in raw_od.ch_names]\n",
        "\n",
        "    # Determine good channels\n",
        "    all_indices = set(range(data.shape[0]))\n",
        "    bad_indices_set = set(bad_indices)\n",
        "    good_indices = sorted(all_indices - bad_indices_set)\n",
        "\n",
        "    if not good_indices:\n",
        "        print(\"All channels are marked as bad. No motion correction applied.\")\n",
        "        return raw_od.copy()\n",
        "\n",
        "\n",
        "    for idx, channel_no in enumerate(good_indices):\n",
        "        # Remove DC component\n",
        "        dc_val = np.mean(data[channel_no, :])\n",
        "        data_no_dc = data[channel_no, :] - dc_val\n",
        "\n",
        "        # Normalize\n",
        "        med_dev = np.median(np.abs(data_no_dc))\n",
        "        if med_dev == 0:\n",
        "            norm_coef = 1\n",
        "            data_normalized = data_no_dc\n",
        "        else:\n",
        "            norm_coef = 1 / (1.4826 * med_dev)\n",
        "            data_normalized = data_no_dc * norm_coef\n",
        "\n",
        "        # Wavelet Transform\n",
        "        coeffs = pywt.wavedec(data_normalized, wavename, mode='periodization', level=n_levels)\n",
        "\n",
        "        # Thresholding (Artifact Removal)\n",
        "        for i in range(1, len(coeffs)):\n",
        "            c = coeffs[i]\n",
        "            q1, q3 = np.percentile(c, [25, 75])\n",
        "            iqr_val = q3 - q1\n",
        "            threshold = iqr * iqr_val\n",
        "            coeffs[i] = np.where((c > (q3 + threshold)) | (c < (q1 - threshold)), 0, c) # np.where(condition, value if true, value if false)\n",
        "\n",
        "        # Inverse Wavelet Transform\n",
        "        reconstructed = pywt.waverec(coeffs, wavename, mode='periodization')\n",
        "\n",
        "        # Denormalize and add DC component\n",
        "        reconstructed = reconstructed / norm_coef + dc_val\n",
        "\n",
        "        # Truncate to original signal length\n",
        "        dod_wavelet[channel_no, :] = reconstructed[:signal_length]\n",
        "\n",
        "    # For bad channels, retain the original data\n",
        "    if bad_indices:\n",
        "        dod_wavelet[bad_indices, :] = data[bad_indices, :]\n",
        "        print(f\"Retained original data for {len(bad_indices)} bad channels.\")\n",
        "\n",
        "    raw_od_corrected = mne.io.RawArray(dod_wavelet, raw_od.info)\n",
        "    return raw_od_corrected"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qj5fBMxt4mX"
      },
      "source": [
        "#### 2.3 hmr_Bandpass_Filter function\n",
        "(Auxiliary --> accelerometer/gyroscope data).\n",
        "Both optical density and auxiliary data uses the same bandpass filter function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xjys4sr3t4mX"
      },
      "outputs": [],
      "source": [
        "def hmr_Bandpass_Filter(data, hpf=0.01, lpf=0.4):\n",
        "    fs = data.info[\"sfreq\"] # sampling frequency = 5.09 Hz\n",
        "    if hpf > (fs/2) or lpf > (fs/2): # if bandpass filter > nyquist frequency\n",
        "        print(\"Warning: Bandpass filter is larger than the Nyquist frequency\")\n",
        "        return data\n",
        "\n",
        "    # Get the data array\n",
        "    x = data.get_data()\n",
        "    filtered_data = x.copy()\n",
        "\n",
        "    # low-pass filter\n",
        "    lpf_norm = lpf / (fs/2) #  The normalized lpf_norm = 0.157 indicates that the low-pass cutoff is at 15.7% of the Nyquist frequency.\n",
        "    if lpf_norm > 0:\n",
        "        # Create filter\n",
        "        filter_order = 3\n",
        "        # Design an 3rd-order digital Butterworth filter and return the filter coefficients.\n",
        "        b, a = signal.butter(filter_order, lpf_norm, 'low')  # butter(N, Wn, btype='low')\n",
        "            # N: The order of the filter\n",
        "            # For digital filters, if fs is not specified, Wn units are normalized from 0 to 1, where 1 is the Nyquist frequency (Wn is thus in half cycles / sample and defined as 2*critical frequencies / fs). If fs is specified, Wn is in the same units as fs.\n",
        "            # btype {‘lowpass’, ‘highpass’, ‘bandpass’, ‘bandstop’}, optional, The type of filter. Default is ‘lowpass’.\n",
        "\n",
        "        for ch in range(x.shape[0]):\n",
        "            filtered_data[ch, :] = signal.filtfilt(b, a, filtered_data[ch, :]) # b : The numerator coefficient vector; a : The denominator coefficient vector\n",
        "\n",
        "    # High-pass filter\n",
        "    hpf_norm = hpf / (fs/2)\n",
        "    if hpf_norm > 0:\n",
        "        filter_order_h = 5\n",
        "        b_h, a_h = signal.butter(filter_order_h,hpf_norm, 'high')\n",
        "        for ch in range(x.shape[0]):\n",
        "            filtered_data[ch, :] = signal.filtfilt(b_h, a_h, filtered_data[ch, :])\n",
        "\n",
        "    data_filtered = data.copy()\n",
        "    data_filtered._data = filtered_data\n",
        "\n",
        "    return data_filtered\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YP7c-yE4t4mX"
      },
      "source": [
        "#### 2.4 Define MotionCorrectCbsi function\n",
        "\n",
        "This function assumes that the true psychological signals of HbO and HbR are negatively correlated\n",
        "\n",
        "Page 5 of https://doi.org/10.3390/s23083979"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-7VC372t4mY"
      },
      "outputs": [],
      "source": [
        "def hmrR_MotionCorrectCbsi(raw_haemo):\n",
        "\n",
        "    hbo_channels = [ch for ch in raw_haemo.ch_names if 'hbo' in ch.lower()]\n",
        "    hbr_channels = [ch for ch in raw_haemo.ch_names if 'hbr' in ch.lower()]\n",
        "\n",
        "    hbo_indices = [idx for idx, ch in enumerate(raw_haemo.ch_names) if 'hbo' in ch.lower()] # [0-47]\n",
        "    hbr_indices = [idx for idx, ch in enumerate(raw_haemo.ch_names) if 'hbr' in ch.lower()] #[48-95]\n",
        "\n",
        "    haemo_data = raw_haemo.get_data() # (96, 2396)\n",
        "    n_timepoints = haemo_data.shape[1] # 2396\n",
        "    n_channels = len(hbo_channels) # 48\n",
        "\n",
        "    data_dc = np.zeros((n_timepoints, 3 ,n_channels)) # (2396, 3, 48)\n",
        "    data_dc[:,0,:] = haemo_data[hbo_indices,:].T\n",
        "    data_dc[:,1,:] = haemo_data[hbr_indices,:].T\n",
        "\n",
        "    # avoid changing original data\n",
        "    dc = data_dc.copy()\n",
        "\n",
        "    # Copy data for correction\n",
        "    dcCbsi = dc.copy()\n",
        "\n",
        "    for idx_ch in range(n_channels):\n",
        "\n",
        "        # Mean-center the HbO and HbR signals\n",
        "        dc_oxy = dc[:, 0, idx_ch] - np.mean(dc[:, 0, idx_ch])\n",
        "        dc_deoxy = dc[:, 1, idx_ch] - np.mean(dc[:, 1, idx_ch])\n",
        "\n",
        "        # Calculate standard deviations\n",
        "        sd_oxy = np.std(dc_oxy, ddof=0) #Delta Degrees of Freedom = 0: divides by n\n",
        "        sd_deoxy = np.std(dc_deoxy, ddof=0)\n",
        "\n",
        "        # Compute alpha, avoiding division by zero\n",
        "        if sd_deoxy != 0:\n",
        "            alfa = sd_oxy / sd_deoxy\n",
        "        else:\n",
        "            alfa = 0\n",
        "\n",
        "        # Apply CBSI correction\n",
        "        dcCbsi[:, 0, idx_ch] = 0.5 * (dc_oxy - alfa * dc_deoxy)\n",
        "        if alfa != 0:\n",
        "            dcCbsi[:, 1, idx_ch] = - (1 / alfa) * dcCbsi[:, 0, idx_ch]\n",
        "        else:\n",
        "            dcCbsi[:, 1, idx_ch] = 0  # Handle division by zero\n",
        "\n",
        "        dcCbsi[:, 2, idx_ch] = dcCbsi[:, 0, idx_ch] + dcCbsi[:, 1, idx_ch]\n",
        "\n",
        "    raw_copy = raw_haemo.copy()\n",
        "\n",
        "    # Step 4: Extract and replace HbO and HbR data\n",
        "    corrected_hbo = dcCbsi[:, 0, :].T  # Shape: (n_channels, n_timepoints)\n",
        "    corrected_hbr = dcCbsi[:, 1, :].T  # Shape: (n_channels, n_timepoints)\n",
        "\n",
        "    # Replace data in-place\n",
        "    raw_copy._data[hbo_indices, :] = corrected_hbo\n",
        "    raw_copy._data[hbr_indices, :] = corrected_hbr\n",
        "\n",
        "    # Step 5: Add HbT channels\n",
        "    corrected_hbt = dcCbsi[:, 2, :].T  # Shape: (n_channels, n_timepoints)\n",
        "\n",
        "    # Define HbT channel names\n",
        "    hbt_channels = [ch.replace('hbo', 'hbt') for ch in hbo_channels]\n",
        "\n",
        "    # Define channel types for HbT\n",
        "    hbt_channel_types = ['misc'] * n_channels  # 'misc' since 'hbt' is not standard in MNE\n",
        "\n",
        "    # Create Info object for HbT channels\n",
        "    hbt_info = mne.create_info(ch_names=hbt_channels, sfreq=raw_copy.info['sfreq'], ch_types=hbt_channel_types)\n",
        "\n",
        "    # Create RawArray for HbT channels\n",
        "    raw_hbt = mne.io.RawArray(corrected_hbt, hbt_info)\n",
        "\n",
        "    # Add HbT channels to the Raw copy\n",
        "    raw_copy.add_channels([raw_hbt], force_update_info=True)\n",
        "\n",
        "    return raw_copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM9IFOo3t4mY"
      },
      "source": [
        "#### 2.5 Define get ROI channel function\n",
        "\n",
        "take away 'hbr' data to reduce redundancy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlKvv7j-t4mY"
      },
      "outputs": [],
      "source": [
        "def get_ROI_channels(raw_copy):\n",
        "    # Get channel names from your fNIRS dataset\n",
        "    channel_names = raw_copy.ch_names\n",
        "\n",
        "    # Define the selection criteria\n",
        "    selection_criteria = ['S2', 'S4', 'S5', 'S6']\n",
        "\n",
        "    # Create a list to store selected channel names\n",
        "    selected_channels = []\n",
        "\n",
        "    for channel in channel_names:\n",
        "        if any(criterion in channel for criterion in selection_criteria) and ('hbo' in channel or 'hbr' in channel or 'hbt' in channel): #HbO channels: 760nm HbR channels: 850nm\n",
        "            selected_channels.append(channel)\n",
        "\n",
        "    # Now, get the indices of these selected channels\n",
        "    picks = [channel_names.index(name) for name in selected_channels]\n",
        "\n",
        "    return picks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LpOWhJht4mY"
      },
      "source": [
        "#### 2.6 Define get evoked metadata for Reappraise / view negative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGOJhHu8t4mY"
      },
      "outputs": [],
      "source": [
        "def get_evoked_metadata(raw_copy, events, event_dict, picks):\n",
        "    # Rejection criteria for bad epochs\n",
        "    reject_criteria = dict(hbo=100e-7)\n",
        "\n",
        "    epochs = mne.Epochs(raw_copy,\n",
        "                       events,\n",
        "                       event_id=event_dict,\n",
        "                       tmin=-2,\n",
        "                       tmax=20,\n",
        "                       picks=picks,\n",
        "                       baseline= (None, 0),# None,\n",
        "                       reject=reject_criteria,\n",
        "                       preload=True,\n",
        "                       verbose=False)\n",
        "\n",
        "    # Get rejected epochs information\n",
        "    drop_log = epochs.drop_log\n",
        "    dropped_indices = [i for i, drops in enumerate(drop_log) if drops]\n",
        "\n",
        "    print(f\"\\nNumber of rejected epochs: {len(dropped_indices)}\")\n",
        "    print(\"\\nRejected epochs event types:\")\n",
        "    for i in dropped_indices:\n",
        "        event_type = events[i][2]\n",
        "        print(f\"Epoch {i}: Event type {event_type}\")\n",
        "\n",
        "\n",
        "    # Separate epochs by condition and channel type\n",
        "    epochs_dict = {\n",
        "        \"Reappraise\": {\n",
        "            \"HbO\": epochs[\"Reappraise\"].get_data(picks=\"hbo\"),\n",
        "\n",
        "        },\n",
        "        \"view_negative\": {\n",
        "            \"HbO\": epochs[\"view negative\"].get_data(picks=\"hbo\"),\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Get channel names for each type\n",
        "    hbo_ch_names = [ch for ch in epochs.ch_names if 'hbo' in ch.lower()]\n",
        "\n",
        "\n",
        "    # Remove frequency encoding from channel names\n",
        "    hbo_ch_names = [ch[:-4] for ch in hbo_ch_names]\n",
        "\n",
        "\n",
        "    # Create metadata DataFrames\n",
        "    metadata_df_reappraise = pd.DataFrame({\n",
        "        'epoch': range(len(epochs[\"Reappraise\"])),\n",
        "        'condition': 'Reappraise',\n",
        "        'times': [epochs.times] * len(epochs[\"Reappraise\"]),\n",
        "        'hbo_ch_names': [hbo_ch_names] * len(epochs[\"Reappraise\"]),\n",
        "    })\n",
        "\n",
        "    metadata_df_view_negative = pd.DataFrame({\n",
        "        'epoch': range(len(epochs[\"view negative\"])),\n",
        "        'condition': 'view_negative',\n",
        "        'times': [epochs.times] * len(epochs[\"view negative\"]),\n",
        "        'hbo_ch_names': [hbo_ch_names] * len(epochs[\"view negative\"]),\n",
        "    })\n",
        "\n",
        "    return epochs_dict, metadata_df_reappraise, metadata_df_view_negative, epochs.times"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQhf4AI9t4mY"
      },
      "source": [
        "#### 2.7 Define interpolation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_23Htii7t4mY"
      },
      "outputs": [],
      "source": [
        "# identifies bad fNIRS channels via scalp coupling index, interpolates them\n",
        "def interpolate_bad_channels(raw_od, sci_value):\n",
        "    # Make a copy for preprocessing\n",
        "    raw_od_copy = raw_od.copy()\n",
        "\n",
        "    # Set montage\n",
        "    montage = raw_od_copy.get_montage()\n",
        "    raw_od_copy.set_montage(montage)\n",
        "\n",
        "    # Calculate scalp coupling index\n",
        "    sci = mne.preprocessing.nirs.scalp_coupling_index(raw_od_copy)\n",
        "\n",
        "    # Mark bad channels based on SCI\n",
        "    raw_od_copy.info[\"bads\"] = list(compress(raw_od_copy.ch_names, sci < sci_value))\n",
        "    # Interpolate bad channels\n",
        "    raw_od_copy.interpolate_bads(\n",
        "        reset_bads=False,      # Keep bad channels in the bads list after interpolation\n",
        "        mode='accurate',       # Use high-quality interpolation\n",
        "        method=\"nearest\"       # Specify interpolation method for fNIRS channels\n",
        "    )\n",
        "    # Create a RawArray with processed data and original info\n",
        "    processed_data = raw_od_copy.get_data()\n",
        "    original_info = raw_od.info.copy()\n",
        "\n",
        "    # Create new Raw object\n",
        "    raw_od = mne.io.RawArray(processed_data, original_info)\n",
        "\n",
        "    return raw_od"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9zYNQmGt4mZ"
      },
      "source": [
        "## 3. Subject Level Analysis\n",
        "\n",
        "Each person has 12 metadata, every metadata is a 2D matrix of shape {39 channels [HbO, HbR, HbT], 113 timepoints [-2 to 20s]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpLBaWAft4mZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import mne\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import mne_nirs\n",
        "import logging  # Import the logging module\n",
        "\n",
        "import warnings\n",
        "\n",
        "# Suppress specific RuntimeWarning about missing nasion\n",
        "warnings.filterwarnings(\"ignore\",message=\"Fiducial point nasion not found, assuming identity unknown to head transformation\",category=RuntimeWarning)\n",
        "\n",
        "# Suppress specific RuntimeWarning about no bad channels to interpolate\n",
        "warnings.filterwarnings(\"ignore\",message=\"No bad channels to interpolate. Doing nothing...\",category=RuntimeWarning)\n",
        "\n",
        "PARTICIPANT_CHANNELS = {\n",
        "1: 'S6_D4', 2: 'S2_D2', 3: 'S2_D3', 4: 'S2_D4', 5: 'S4_D6',\n",
        "6: 'S6_D7', 7: 'S6_D4', 8: 'S4_D6', 9: 'S5_D3', 10: 'S4_D4',\n",
        "11: 'S4_D4', 12: 'S5_D4', 13: 'S2_D2', 14: 'S1_D2', 15: 'S5_D3',\n",
        "16: 'S4_D2', 17: 'S6_D4', 18: 'S6_D6', 19: 'S4_D6', 20: 'S1_D2',\n",
        "21: 'S5_D4', 22: 'S1_D2', 23: 'S2_D4', 24: 'S6_D4', 25: 'S6_D4',\n",
        "26: 'S6_D4', 27: 'S2_D4', 28: 'S6_D6', 29: 'S4_D6', 30: 'S2_D4',\n",
        "31: 'S6_D4', 32: 'S5_D7', 33: 'S6_D4', 34: 'S4_D6', 35: 'S6_D4',\n",
        "36: 'S6_D7', 37: 'S4_D4', 38: 'S2_D4', 39: 'S4_D6', 40: 'S6_D4',\n",
        "41: 'S4_D2', 42: 'S2_D4', 43: 'S4_D4', 44: 'S4_D4', 45: 'S5_D3',\n",
        "46: 'S6_D4', 47: 'S4_D6', 48: 'S4_D2', 49: 'S4_D4', 50: 'S4_D6',\n",
        "51: 'S4_D6', 52: 'S4_D6', 53: 'S4_D2', 54: 'S6_D4', 55: 'S4_D2',\n",
        "56: 'S6_D6', 57: 'S4_D2'\n",
        "}\n",
        "\n",
        "PARTICIPANT_CHANNELS_2 = {\n",
        "    1: 'S6_D6', 2: 'S1_D2', 3: 'S6_D7', 4: 'S2_D3', 5: 'S6_D6',\n",
        "    6: 'S6_D4', 7: 'S6_D6', 8: 'S5_D3', 9: 'S5_D7', 10: 'S6_D4',\n",
        "    11: 'S4_D6', 12: 'S5_D3', 13: 'S4_D2', 14: 'S4_D4', 15: 'S5_D7',\n",
        "    16: 'S6_D4', 17: 'S4_D4', 18: 'S6_D4', 19: 'S6_D6', 20: 'S2_D2',\n",
        "    21: 'S2_D4', 22: 'S2_D4', 23: 'S5_D4', 24: 'S2_D4', 25: 'S6_D7',\n",
        "    26: 'S4_D4', 27: 'S4_D4', 28: 'S4_D6', 29: 'S4_D4', 30: 'S2_D4',\n",
        "    31: 'S6_D7', 32: 'S5_D3', 33: 'S6_D6', 34: 'S4_D2', 35: 'S6_D6',\n",
        "    36: 'S5_D7', 37: 'S5_D3', 38: 'S4_D4', 39: 'S6_D4', 40: 'S4_D6',\n",
        "    41: 'S4_D4', 42: 'S2_D3', 43: 'S4_D6', 44: 'S6_D4', 45: 'S5_D7',\n",
        "    46: 'S6_D6', 47: 'S6_D6', 48: 'S4_D4', 49: 'S5_D4', 50: 'S6_D4',\n",
        "    51: 'S6_D4', 52: 'S4_D4', 53: 'S4_D6', 54: 'S5_D4', 55: 'S6_D4',\n",
        "    56: 'S4_D6', 57: 'S4_D4'\n",
        "}\n",
        "\n",
        "Best_performing_channels = {\n",
        "1: 'S6_D4', 2: 'S2_D2', 3: 'S6_D7', 4: 'S2_D4', 5: 'S4_D6',\n",
        "6: 'S6_D4', 7: 'S6_D4', 8: 'S5_D3', 9: 'S5_D7', 10: 'S6_D4',\n",
        "11: 'S4_D6', 12: 'S5_D3', 13: 'S2_D2', 14: 'S1_D2', 15: 'S5_D3',\n",
        "16: 'S4_D2', 17: 'S4_D4', 18: 'S6_D4', 19: 'S4_D6', 20: 'S2_D2',\n",
        "21: 'S5_D4', 22: 'S2_D4', 23: 'S2_D4', 24: 'S4_D4', 25: 'S4_D4',\n",
        "26: 'S6_D4', 27: 'S2_D4', 28: 'S6_D6', 29: 'S4_D6', 30: 'S2_D4',\n",
        "31: 'S6_D4', 32: 'S5_D7', 33: 'S6_D6', 34: 'S4_D6', 35: 'S6_D4',\n",
        "36: 'S6_D7', 37: 'S5_D3', 38: 'S4_D4', 39: 'S4_D6', 40: 'S6_D4',\n",
        "41: 'S4_D2', 42: 'S2_D4', 43: 'S4_D4', 44: 'S4_D4', 45: 'S5_D3',\n",
        "46: 'S6_D4', 47: 'S4_D6', 48: 'S4_D2', 49: 'S4_D4', 50: 'S6_D4',\n",
        "51: 'S4_D6', 52: 'S4_D6', 53: 'S4_D2', 54: 'S6_D4', 55: 'S4_D2',\n",
        "56: 'S6_D6', 57: 'S4_D2'\n",
        "}\n",
        "\n",
        "base_dir = r\"/content/drive/MyDrive/fNIRS data/neurofeedback\" # google colab: r\"/content/drive/MyDrive/fNIRS data/neurofeedback\"\n",
        "participant_id = 24 # For participant p01\n",
        "\n",
        "def subject_wise_analysis_1_to_6(base_dir, participant_id, ROI=True):\n",
        "\n",
        "    \"\"\"\n",
        "    Process fNIRS data for a given participant across multiple runs.\n",
        "\n",
        "    Parameters:\n",
        "    - base_dir (str): Base directory containing participant data.\n",
        "    - participant_id (int): Participant identifier.\n",
        "\n",
        "    Returns:\n",
        "    - all_run_results (list): List of dictionaries containing run data.\n",
        "    \"\"\"\n",
        "\n",
        "    participant_dir = os.path.join(base_dir, f\"p{participant_id:02d}\") # 0 means add zeros if needed # 2 specifies a minimum field width of 2 characters # d indicates that the valueshould be formatted as a decimal integer(base-10 number system)\n",
        "\n",
        "    # Find all directories that have at least one underscore in their name\n",
        "    date_dirs = glob(os.path.join(participant_dir, \"????-??-??_???\")) # ['C:\\\\Users\\\\user\\\\Desktop\\\\fNIRS data\\\\p01\\\\2024-08-09_001', 'C:\\\\Users\\\\user\\\\Desktop\\\\fNIRS data\\\\p01\\\\2024-08-09_002', 'C:\\\\Users\\\\user\\\\Desktop\\\\fNIRS data\\\\p01\\\\2024-08-09_003', 'C:\\\\Users\\\\user\\\\Desktop\\\\fNIRS data\\\\p01\\\\2024-08-09_004', 'C:\\\\Users\\\\user\\\\Desktop\\\\fNIRS data\\\\p01\\\\2024-08-09_005', 'C:\\\\Users\\\\user\\\\Desktop\\\\fNIRS data\\\\p01\\\\2024-08-09_006']\n",
        "    # glob : find all file system paths matching a specified pattern\n",
        "    # the pattern \"*_*\": * indicates any number of characters, _ matches an underscore,\n",
        "    if not date_dirs:\n",
        "        print(f\"No data directories found for participant {participant_id}\")\n",
        "        return None\n",
        "\n",
        "    target_channel = PARTICIPANT_CHANNELS[participant_id]\n",
        "\n",
        "    target_channel_2 = PARTICIPANT_CHANNELS_2[participant_id]\n",
        "\n",
        "    target_channel_3 = Best_performing_channels[participant_id]\n",
        "\n",
        "    # create a dictionary to store HbO data from different runs\n",
        "    all_epochs_dict = {\n",
        "    'neuro_train_1': {\"Reappraise_HbO\": []},\n",
        "    'neuro_train_2': {\"Reappraise_HbO\": []},\n",
        "    'neuro_train_3': {\"Reappraise_HbO\": []},\n",
        "    'neuro_train_4': {\"Reappraise_HbO\": []}\n",
        "    }\n",
        "\n",
        "    # Extract run numbers and create a dictionary mapping run numbers to full paths\n",
        "    run_dict = {int(run[-3:]): run for run in date_dirs}\n",
        "\n",
        "    # Sort run numbers\n",
        "    all_runs = sorted(run_dict.keys())\n",
        "\n",
        "    # Get middle runs (exclude first and last)\n",
        "    neurofeedback_training = all_runs[1:-1]\n",
        "\n",
        "    # Create list of selected runs in order\n",
        "    selected_runs = [run_dict[run_num] for run_num in neurofeedback_training]\n",
        "\n",
        "    for idx, run in enumerate(selected_runs):\n",
        "\n",
        "        run_name = f'neuro_train_{idx+1}'\n",
        "        # print(f\"\\nProcessing Run {run[-3:]} for participant {participant_id} as {run_name}\")  # Gets last 3 characters (e.g., '001')\n",
        "        snirf_files = glob(os.path.join(run,\"*.snirf\"))\n",
        "\n",
        "        if not snirf_files:\n",
        "            print(f\"No SNIRF file found in {run}\")\n",
        "            continue\n",
        "\n",
        "        fnirs_snirf = snirf_files[0]  # Take the first (and hopefully only) SNIRF file\n",
        "\n",
        "        # Load raw intensity info from .snirf file\n",
        "        raw_intensity = mne.io.read_raw_snirf(fnirs_snirf, preload=True)\n",
        "        raw_intensity.load_data()\n",
        "\n",
        "         # montage is a description of the sensor positionin 3D space\n",
        "\n",
        "        # drop unwanted events (only leave 2 and 3), and annotate event names\n",
        "        raw_intensity = process_annotations(raw_intensity)\n",
        "\n",
        "        # Remove channels that are too close to each other (less than 1cm)\n",
        "        raw_intensity = remove_close_channels(raw_intensity)\n",
        "\n",
        "        # Convert signal to optical density\n",
        "        raw_od = mne.preprocessing.nirs.optical_density(raw_intensity)\n",
        "\n",
        "        # identifies bad fNIRS channels via scalp coupling index, interpolates them\n",
        "        raw_od = interpolate_bad_channels(raw_od, sci_value = 0.9)\n",
        "\n",
        "        # Motion Correction Wavelet\n",
        "        raw_od_wavelet = hmr_motion_correct_wavelet_standard(raw_od, iqr=1.5, turnon=1, wavename='db2')\n",
        "\n",
        "        # Apply Bandpass Filter\n",
        "        raw_od_wavelet_filtered = hmr_Bandpass_Filter(raw_od_wavelet, hpf=0.01, lpf=0.4)\n",
        "\n",
        "        # Convert to haemoglobin, and apply further data cleaning techniques\n",
        "        raw_haemo = mne.preprocessing.nirs.beer_lambert_law(raw_od_wavelet_filtered, ppf=6.0) # default in Homer\n",
        "\n",
        "        # Apply Motion Correct Cbsi\n",
        "        raw_copy = hmrR_MotionCorrectCbsi(raw_haemo) # OR use library function: mne_nirs.signal_enhancement.enhance_negative_correlation(raw_haemo)\n",
        "\n",
        "        target_channel_hbo = f\"{target_channel} hbo\" # HbO = 760nm\n",
        "        target_channel_hbo_2 = f\"{target_channel_2} hbo\"\n",
        "        target_channel_hbo_3 = f\"{target_channel_3} hbo\"\n",
        "\n",
        "        # Get channel index\n",
        "        ch_idx = raw_copy.ch_names.index(target_channel_hbo)\n",
        "        ch_idx_2 = raw_copy.ch_names.index(target_channel_hbo_2)\n",
        "        ch_idx_3 = raw_copy.ch_names.index(target_channel_hbo_3)\n",
        "\n",
        "        # Find event epochs (8 view_negative / 8 reappraise)\n",
        "        events, event_dict = mne.events_from_annotations(raw_intensity)\n",
        "\n",
        "        epochs_dict, metadata_df_reappraise, metadata_df_view_negative, epochs_times = get_evoked_metadata(raw_copy, events, event_dict, picks=[ch_idx_3])  # picks=[ch_idx, ch_idx_2]\n",
        "\n",
        "        reappraise_data = epochs_dict[\"Reappraise\"][\"HbO\"] # Shape: (8, 1, 113)\n",
        "\n",
        "        # Debug print to see the shape of reappraise_data\n",
        "        # print(f\"Raw reappraise data shape: {reappraise_data.shape}\")\n",
        "\n",
        "        # mean_data_ch1 = np.mean(reappraise_data[:, 0, :], axis=(0, 1))  # First channel\n",
        "        # mean_data_ch2 = np.mean(reappraise_data[:, 1, :], axis=(0, 1))  # Second channel\n",
        "\n",
        "        mean_data = np.mean(reappraise_data, axis=(0, 2)) # return a single value across epochs and timepoints\n",
        "        # mean_data = np.mean([mean_data_ch1, mean_data_ch2], axis=0)\n",
        "\n",
        "        #if np.isscalar(mean_data):\n",
        "        #      mean_data = np.array([mean_data])  # Convert scalar to 1D array\n",
        "        # print(f\"mean reappraise data shape: {mean_data.shape}\")\n",
        "        # Store only Reappraise HbO data for this specific run\n",
        "        all_epochs_dict[run_name][\"Reappraise_HbO\"] = mean_data\n",
        "\n",
        "        print(f\"Participant {participant_id} - {run_name} - {target_channel_3}: \"\n",
        "              f\"Value = {all_epochs_dict[run_name]['Reappraise_HbO']}\")\n",
        "    '''\n",
        "        # After each major preprocessing step\n",
        "    print(\"After OD conversion:\")\n",
        "    print(raw_od.get_data().mean())\n",
        "\n",
        "    print(\"After wavelet:\")\n",
        "    print(raw_od_wavelet.get_data().mean())\n",
        "\n",
        "    print(\"After filtering:\")\n",
        "    print(raw_od_wavelet_filtered.get_data().mean())\n",
        "\n",
        "    print(\"After Beer-Lambert:\")\n",
        "    print(raw_haemo.get_data().mean())\n",
        "\n",
        "    print(\"After CBSI:\")\n",
        "    print(raw_copy.get_data().mean())\n",
        "    '''\n",
        "    return all_epochs_dict, epochs_times, target_channel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffgV8f-Kt4mZ"
      },
      "outputs": [],
      "source": [
        "all_epochs_dict, epochs_time, hbo_ch_names =subject_wise_analysis_1_to_6(base_dir, participant_id, ROI=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_epochs_dict)"
      ],
      "metadata": {
        "id": "fvFAAQzseDDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJ9xtAWEt4ma"
      },
      "source": [
        "## 4. get all participants' data\n",
        "\n",
        "1. If a channel is bad in any run, it might be unreliable in other runs too\n",
        "2. For group analysis, we need consistent channel layouts across all runs\n",
        "3. So, we remove channels marked as bad in any run from all runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iwm2Jeht4ma"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import stats  # Add this import\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def process_fnirs_data():\n",
        "\n",
        "    # We reject participant's data if the negativity reaponse pattern happens to be \"invariant responding\", which is a clear indicator of low-quality data\n",
        "\n",
        "    base_dir = r\"/content/drive/MyDrive/fNIRS data/neurofeedback\" # google colab: r\"/content/drive/MyDrive/fNIRS data/neurofeedback\"\n",
        "    participant_ids= [1,3,6,7,10,11,13,15,16,17,20,21,24,28,30,31,33,37,38,40,41,43,45,46,48,50,55,56] # google colab: [1,3,6,7,8,10,11,13,15,16,17,20,21,24,28,30,31,33,37,38,40,41,43,45]\n",
        "      # reject p8 as 51/63 responses are 1s (79.7%), 13/64 responses are 2s\n",
        "\n",
        "    # Initialize group-level epochs dictionary\n",
        "    group_epochs_dict = {\n",
        "    'neuro_train_1': {\"Reappraise_HbO\": []}, # [] is an empty list\n",
        "    'neuro_train_2': {\"Reappraise_HbO\": []},\n",
        "    'neuro_train_3': {\"Reappraise_HbO\": []},\n",
        "    'neuro_train_4': {\"Reappraise_HbO\": []}\n",
        "    }\n",
        "\n",
        "    for participant_id in participant_ids:\n",
        "        print(f\"\\nProcessing participant {participant_id}\")\n",
        "\n",
        "        participant_epochs,_,_=subject_wise_analysis_1_to_6(base_dir, participant_id, ROI=True)\n",
        "\n",
        "        for condition in group_epochs_dict:\n",
        "                if participant_epochs[condition]['Reappraise_HbO'].size > 0:  # Check if there's data\n",
        "                    # Use append instead of extend for single float values\n",
        "                    group_epochs_dict[condition]['Reappraise_HbO'].append(\n",
        "                    float(participant_epochs[condition]['Reappraise_HbO'][0])) # add value to the list one by one\n",
        "\n",
        "    # Convert lists to numpy arrays for analysis\n",
        "    for run in group_epochs_dict:\n",
        "        group_epochs_dict[run]['Reappraise_HbO'] = np.array(group_epochs_dict[run]['Reappraise_HbO'])\n",
        "\n",
        "    return group_epochs_dict\n",
        "\n",
        "group_epochs_dict = process_fnirs_data()\n",
        "\n",
        "'''\n",
        "    # Prepare data for statistical analysis\n",
        "    run_data = {}\n",
        "    for run in group_epochs_dict:\n",
        "        run_data[run] = group_epochs_dict[run]['Reappraise_HbO']\n",
        "\n",
        "    # Calculate descriptive statistics\n",
        "    descriptive_stats = {\n",
        "        run: {\n",
        "            'mean': np.mean(data),\n",
        "            'std': np.std(data),\n",
        "            'sem': stats.sem(data),\n",
        "            'n': len(data)\n",
        "        } for run, data in run_data.items()\n",
        "    }\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "group_epochs_dict"
      ],
      "metadata": {
        "id": "Zg1BIPQ0ay8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Perform repeated-measure ANOVA"
      ],
      "metadata": {
        "id": "H0qZVYan6hj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.anova import AnovaRM\n",
        "\n",
        "def perform_repeated_anova(group_epochs_dict):\n",
        "\n",
        "    total_participants = len(group_epochs_dict['neuro_train_1']['Reappraise_HbO'])\n",
        "\n",
        "    total_runs = ['neuro_train_1', 'neuro_train_2', 'neuro_train_3', 'neuro_train_4']\n",
        "    all_subjects_data = []\n",
        "    for participant_id in range(total_participants):\n",
        "        subject_data = []\n",
        "        for current_run_name in total_runs:\n",
        "              run_number = current_run_name[-1]\n",
        "              hbo_value = group_epochs_dict[current_run_name]['Reappraise_HbO'][participant_id]\n",
        "              subject_data.append({\n",
        "                  'Subject': participant_id + 1,\n",
        "                  'Run': run_number,\n",
        "                  'HbO': hbo_value\n",
        "              })\n",
        "        all_subjects_data.extend(subject_data)\n",
        "    # For each subject's data in each run\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df_long = pd.DataFrame(all_subjects_data)\n",
        "\n",
        "    print(\"First few rows (showing first subject across all runs):\")\n",
        "    print(df_long.head(4))\n",
        "\n",
        "    # Perform repeated measures ANOVA\n",
        "    aovrm = AnovaRM(data=df_long, depvar='HbO', subject='Subject', within=['Run'])\n",
        "    result = aovrm.fit()\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nANOVA Results:\")\n",
        "    print(result.summary())\n",
        "\n",
        "    return result, df_long\n",
        "\n",
        "result, df = perform_repeated_anova(group_epochs_dict)"
      ],
      "metadata": {
        "id": "2IPKUQCS6lm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run pairwise T-test"
      ],
      "metadata": {
        "id": "Zej9PJQfjr2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "# Extract data from each training session\n",
        "train1 = group_epochs_dict['neuro_train_1']['Reappraise_HbO']\n",
        "train2 = group_epochs_dict['neuro_train_2']['Reappraise_HbO']\n",
        "train3 = group_epochs_dict['neuro_train_3']['Reappraise_HbO']\n",
        "train4 = group_epochs_dict['neuro_train_4']['Reappraise_HbO']\n",
        "\n",
        "# Create list of all training data\n",
        "training_data = [train1, train2, train3, train4]\n",
        "training_names = ['Train 1', 'Train 2', 'Train 3', 'Train 4']\n",
        "\n",
        "# Perform paired t-tests for all combinations\n",
        "print(\"Paired t-test results:\\n\")\n",
        "print(\"Comparison\\t\\tt-statistic\\tp-value\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for (data1, name1), (data2, name2) in combinations(zip(training_data, training_names), 2):\n",
        "    t_stat, p_val = stats.ttest_rel(data1, data2)\n",
        "    print(f\"{name1} vs {name2}\\t\\t{t_stat:.4f}\\t\\t{p_val:.4f}\")"
      ],
      "metadata": {
        "id": "z-EMCD4JjuAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize t-test results"
      ],
      "metadata": {
        "id": "D99nYccV4dJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from itertools import combinations\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# Extract and scale the data (multiply by 10^7 to make values more readable)\n",
        "train1 = group_epochs_dict['neuro_train_1']['Reappraise_HbO'] * 1e7\n",
        "train2 = group_epochs_dict['neuro_train_2']['Reappraise_HbO'] * 1e7\n",
        "train3 = group_epochs_dict['neuro_train_3']['Reappraise_HbO'] * 1e7\n",
        "train4 = group_epochs_dict['neuro_train_4']['Reappraise_HbO'] * 1e7\n",
        "\n",
        "training_data = [train1, train2, train3, train4]\n",
        "training_names = ['Train 1', 'Train 2', 'Train 3', 'Train 4']\n",
        "\n",
        "# Calculate p-values from paired t-tests\n",
        "p_values = []\n",
        "comparisons = []\n",
        "for (data1, name1), (data2, name2) in combinations(zip(training_data, training_names), 2):\n",
        "    t_stat, p_val = stats.ttest_rel(data1, data2)\n",
        "    p_values.append(p_val)\n",
        "    comparisons.append((name1, name2))\n",
        "\n",
        "# Create figure and axis\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "# Add title\n",
        "plt.title('Mean FNIRS HbO Response across Neurofeedback Training', pad=20)\n",
        "\n",
        "# Set up the plot parameters with darker colors\n",
        "x_positions = [1, 2, 3, 4]\n",
        "colors = ['darkorange', 'royalblue', 'forestgreen', 'purple']\n",
        "\n",
        "# Calculate means for each training session\n",
        "means = [np.mean(train) for train in training_data]\n",
        "\n",
        "# Plot horizontal mean lines and values\n",
        "for i, (x, mean, color) in enumerate(zip(x_positions, means, colors)):\n",
        "    ax.hlines(mean, x-0.2, x+0.2, colors=color, linewidth=2)\n",
        "    ax.text(x+0.3, mean, f'{mean:.2f}', color=color)\n",
        "\n",
        "# Plot individual data points\n",
        "for i, (x, train_data, color) in enumerate(zip(x_positions, training_data, colors)):\n",
        "    # Create a small jitter for x-positions to avoid overlap\n",
        "    x_jitter = np.random.normal(0, 0.05, size=len(train_data))\n",
        "    ax.scatter(x + x_jitter, train_data, color=color, alpha=0.6, s=50)\n",
        "\n",
        "# Function to format p-value with 2 significant figures\n",
        "def format_pvalue(p_value):\n",
        "    if p_value < 0.001:\n",
        "        return f\"p<0.001\"\n",
        "    # Convert to string in scientific notation with 1 decimal place (2 sig figs)\n",
        "    p_str = f\"{p_value:.1e}\"\n",
        "    # Split into base and exponent\n",
        "    base, exponent = p_str.split('e')\n",
        "    # Convert back to decimal notation\n",
        "    p_formatted = float(base) * 10**float(exponent)\n",
        "    # Format to ensure 2 significant figures\n",
        "    if p_formatted < 0.01:\n",
        "        return f\"p={p_formatted:.2e}\"\n",
        "    elif p_formatted < 0.1:\n",
        "        return f\"p={p_formatted:.3f}\"\n",
        "    else:\n",
        "        return f\"p={p_formatted:.2f}\"\n",
        "\n",
        "# Add significance brackets\n",
        "def add_significance_bracket(x1, x2, y, p_value):\n",
        "    ax.hlines(y, x1, x2, color='black')\n",
        "    ax.vlines([x1, x2], y-0.2, y, color='black')\n",
        "    p_text = format_pvalue(p_value)\n",
        "    text = ax.text((x1+x2)/2, y+0.2, p_text, ha='center', va='bottom')\n",
        "\n",
        "    # Add yellow background for significant p-values\n",
        "    if p_value < 0.05:\n",
        "        # Get the bounding box of the text\n",
        "        bbox = text.get_window_extent(renderer=fig.canvas.get_renderer())\n",
        "        # Transform from display to data coordinates\n",
        "        bbox_data = bbox.transformed(ax.transData.inverted())\n",
        "        # Create a rectangle patch with yellow background\n",
        "        rect = patches.Rectangle((bbox_data.x0, bbox_data.y0),\n",
        "                               bbox_data.width, bbox_data.height,\n",
        "                               facecolor='yellow',\n",
        "                               alpha=1.0,\n",
        "                               transform=ax.transData)\n",
        "        # Add the rectangle patch to the plot\n",
        "        ax.add_patch(rect)\n",
        "        # Bring text to front\n",
        "        text.set_zorder(rect.get_zorder() + 1)\n",
        "\n",
        "# Calculate appropriate y-positions for brackets based on data range\n",
        "data_max = max([np.max(train) for train in training_data])\n",
        "data_min = min([np.min(train) for train in training_data])\n",
        "data_range = data_max - data_min\n",
        "\n",
        "# Increase spacing between brackets\n",
        "bracket_start = data_max + data_range*0.2\n",
        "bracket_spacing = data_range*0.25  # Increased spacing between brackets\n",
        "bracket_heights = [bracket_start + i*bracket_spacing for i in range(len(comparisons))]\n",
        "\n",
        "# Add brackets for each comparison using calculated p-values\n",
        "for (name1, name2), p_val, y_pos in zip(comparisons, p_values, bracket_heights):\n",
        "    x1 = training_names.index(name1) + 1\n",
        "    x2 = training_names.index(name2) + 1\n",
        "    add_significance_bracket(x1, x2, y_pos, p_val)\n",
        "\n",
        "# Customize the plot\n",
        "ax.set_xlim(0.5, 4.5)\n",
        "ax.set_xticks(x_positions)\n",
        "ax.set_xticklabels(training_names)\n",
        "ax.set_ylabel('HbO Response (×10⁻⁷)')\n",
        "ax.set_xlabel('Neurofeedback')\n",
        "\n",
        "# Add gridlines an"
      ],
      "metadata": {
        "id": "Dpmo6BBmzF_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run a more simple t-test, diving the data into run 1+2 vs run 3+4"
      ],
      "metadata": {
        "id": "30g_1YJfj_Kj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "import numpy as np\n",
        "\n",
        "# Extract data from each training session\n",
        "train1 = group_epochs_dict['neuro_train_1']['Reappraise_HbO']\n",
        "train2 = group_epochs_dict['neuro_train_2']['Reappraise_HbO']\n",
        "train3 = group_epochs_dict['neuro_train_3']['Reappraise_HbO']\n",
        "train4 = group_epochs_dict['neuro_train_4']['Reappraise_HbO']\n",
        "\n",
        "# Calculate means for early (1+2) and late (3+4) sessions for each subject\n",
        "early_sessions = (train1 + train2) / 2\n",
        "late_sessions = (train3 + train4) / 2\n",
        "\n",
        "# Perform paired t-test\n",
        "t_stat, p_val = stats.ttest_rel(early_sessions, late_sessions)\n",
        "\n",
        "print(\"Paired t-test results:\")\n",
        "print(f\"Early (1+2) vs Late (3+4) sessions:\")\n",
        "print(f\"t-statistic: {t_stat:.4f}\")\n",
        "print(f\"p-value: {p_val:.4f}\")\n",
        "\n",
        "# Optional: Calculate mean and std for each group\n",
        "print(\"\\nDescriptive Statistics:\")\n",
        "print(f\"Early sessions mean: {np.mean(early_sessions):.2e}\")\n",
        "print(f\"Early sessions std: {np.std(early_sessions):.2e}\")\n",
        "print(f\"Late sessions mean: {np.mean(late_sessions):.2e}\")\n",
        "print(f\"Late sessions std: {np.std(late_sessions):.2e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9MfECy6kHqO",
        "outputId": "6675c62b-ae73-46d0-a739-398c9e03480e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paired t-test results:\n",
            "Early (1+2) vs Late (3+4) sessions:\n",
            "t-statistic: -1.1190\n",
            "p-value: 0.2730\n",
            "\n",
            "Descriptive Statistics:\n",
            "Early sessions mean: 2.24e-08\n",
            "Early sessions std: 7.49e-08\n",
            "Late sessions mean: 4.38e-08\n",
            "Late sessions std: 1.09e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the t-test"
      ],
      "metadata": {
        "id": "p4nZJP1G5CIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# Calculate means for early (1+2) and late (3+4) sessions for each subject\n",
        "early_sessions = (train1 + train2) / 2\n",
        "late_sessions = (train3 + train4) / 2\n",
        "\n",
        "# Scale the data (multiply by 10^7 to make values more readable)\n",
        "early_sessions = early_sessions * 1e7\n",
        "late_sessions = late_sessions * 1e7\n",
        "\n",
        "# Perform paired t-test\n",
        "t_stat, p_val = stats.ttest_rel(early_sessions, late_sessions)\n",
        "\n",
        "# Create figure and axis\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "\n",
        "# Add title\n",
        "plt.title('Mean FNIRS HbO Response: Early vs Late Training Sessions', pad=20)\n",
        "\n",
        "# Set up the plot parameters\n",
        "x_positions = [1, 2]\n",
        "colors = ['darkorange', 'royalblue']\n",
        "names = ['Early\\n(Train 1-2)', 'Late\\n(Train 3-4)']\n",
        "\n",
        "# Calculate means\n",
        "means = [np.mean(early_sessions), np.mean(late_sessions)]\n",
        "data = [early_sessions, late_sessions]\n",
        "\n",
        "# Plot horizontal mean lines and values\n",
        "for i, (x, mean, color) in enumerate(zip(x_positions, means, colors)):\n",
        "    ax.hlines(mean, x-0.2, x+0.2, colors=color, linewidth=2)\n",
        "    ax.text(x+0.3, mean, f'{mean:.2f}', color=color)\n",
        "\n",
        "# Plot individual data points\n",
        "for i, (x, session_data, color) in enumerate(zip(x_positions, data, colors)):\n",
        "    # Create a small jitter for x-positions to avoid overlap\n",
        "    x_jitter = np.random.normal(0, 0.05, size=len(session_data))\n",
        "    ax.scatter(x + x_jitter, session_data, color=color, alpha=0.6, s=50)\n",
        "\n",
        "# Function to format p-value with 2 significant figures\n",
        "def format_pvalue(p_value):\n",
        "    if p_value < 0.001:\n",
        "        return f\"p<0.001\"\n",
        "    # Convert to string in scientific notation with 1 decimal place (2 sig figs)\n",
        "    p_str = f\"{p_value:.1e}\"\n",
        "    # Split into base and exponent\n",
        "    base, exponent = p_str.split('e')\n",
        "    # Convert back to decimal notation\n",
        "    p_formatted = float(base) * 10**float(exponent)\n",
        "    # Format to ensure 2 significant figures\n",
        "    if p_formatted < 0.01:\n",
        "        return f\"p={p_formatted:.2e}\"\n",
        "    elif p_formatted < 0.1:\n",
        "        return f\"p={p_formatted:.3f}\"\n",
        "    else:\n",
        "        return f\"p={p_formatted:.2f}\"\n",
        "\n",
        "# Calculate appropriate y-positions for bracket based on data range\n",
        "data_max = max(np.max(early_sessions), np.max(late_sessions))\n",
        "data_min = min(np.min(early_sessions), np.min(late_sessions))\n",
        "data_range = data_max - data_min\n",
        "\n",
        "# Add significance bracket\n",
        "def add_significance_bracket(x1, x2, y, p_value):\n",
        "    ax.hlines(y, x1, x2, color='black')\n",
        "    ax.vlines([x1, x2], y-0.2, y, color='black')\n",
        "    p_text = format_pvalue(p_value)\n",
        "    text = ax.text((x1+x2)/2, y+0.2, p_text, ha='center', va='bottom')\n",
        "\n",
        "    # Add yellow background for significant p-values\n",
        "    if p_value < 0.05:\n",
        "        # Get the bounding box of the text\n",
        "        bbox = text.get_window_extent(renderer=fig.canvas.get_renderer())\n",
        "        # Transform from display to data coordinates\n",
        "        bbox_data = bbox.transformed(ax.transData.inverted())\n",
        "        # Create a rectangle patch with yellow background\n",
        "        rect = patches.Rectangle((bbox_data.x0, bbox_data.y0),\n",
        "                               bbox_data.width, bbox_data.height,\n",
        "                               facecolor='yellow',\n",
        "                               alpha=1,\n",
        "                               transform=ax.transData)\n",
        "        # Add the rectangle patch to the plot\n",
        "        ax.add_patch(rect)\n",
        "        # Bring text to front\n",
        "        text.set_zorder(rect.get_zorder() + 1)\n",
        "\n",
        "# Add bracket\n",
        "bracket_height = data_max + data_range*0.2\n",
        "add_significance_bracket(1, 2, bracket_height, p_val)\n",
        "\n",
        "# Customize the plot\n",
        "ax.set_xlim(0.5, 2.5)\n",
        "ax.set_xticks(x_positions)\n",
        "ax.set_xticklabels(names)\n",
        "ax.set_ylabel('HbO Response (×10⁻⁷)')\n",
        "ax.set_xlabel('Neurofeedback Training Sessions')\n",
        "\n",
        "# Add gridlines and remove top and right spines\n",
        "ax.grid(True, axis='y', linestyle='--', alpha=0.3)\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "\n",
        "# Set y-limits\n",
        "y_min = data_min - data_range*0.1\n",
        "y_max = bracket_height + data_range*0.3\n",
        "ax.set_ylim(y_min, y_max)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o9QRnf0QkvTD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "q8463p6_t4mT"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}